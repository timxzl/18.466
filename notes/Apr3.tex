\documentclass[11pt]{article}
\usepackage{amsmath, graphicx, amsfonts}
\def\thestudent{Zhilei Xu (929552018)}
\usepackage{fancyhdr}
\usepackage{lastpage}
\pagestyle{fancy}
\fancyhf{}
\fancyhf[HLEO]{\thestudent}
\fancyhf[HCEO]{18.466 - Lecture Notes}
\fancyhf[HREO]{Page \thepage\ of \pageref{LastPage}}
\renewcommand\headrulewidth{0.4pt}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand\erf{\textrm{erf}}
\newcommand\sgn{\textrm{sgn}}
\newcommand{\widesim}[2][1.5]{
  \mathrel{\overset{#2}{\scalebox{#1}[1]{$\sim$}}}
}
\newcommand\eqnlabel[1]{\label{eqn:#1}}
\newcommand\eqnref[1]{(\ref{eqn:#1})}
\newcommand{\Beta}{\mathcal{B}}
\begin{document}
MLE  E-M algorithm

e.g. Normal Mixture Model
\\
Five parameters $(\alpha, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2)$.
$x_i \sim N(\mu_1, \sigma_1^2)$ with probability $\alpha$,
$x_i \sim N(\mu_2, \sigma_2^2)$ with probability $1-\alpha$.

Let
$$
Y_i = \left\{\begin{array}{ll}
1 & X_i \sim N(\mu_1, \sigma_1^2)
\\
2 & X_i \sim N(\mu_2, \sigma_2^2)
\end{array}\right.
$$
if we observe $Y_i$.

$$
L(\theta | x, y) = f(x, y | \theta) =
\prod_{i=1}^{n} (\alpha)\phi(x_i | \mu_1, \sigma_1^2)^{2-y_i}
(1-\alpha)\phi(x_i | \mu_2, \sigma_2^2)^{y_i-1}
$$
$$
l(\theta | x, y) = \sum_{i=1}^{n} (2-y_i)\log \alpha \phi(x_i | \mu_1, \sigma_1^2) + \sum_{i=1}^{n} (y_i-1) \log (1-\alpha) \phi(x_i | \mu_2, \sigma_2^2)
$$
$$
\left\{\begin{array}{ll}
L(\theta | x, y)
\\
l(\theta | x, y)
\end{array}\right.
$$
are random variables $h(Y | X, \theta)$.

$\theta^{(0)} = (\alpha^{(0)}, \mu_1^{(0)}, \dots)$.
\begin{enumerate}
\item Expectation.
Calculate
$
Q(\theta, \theta^{(i-1)}) =
E[ l(\theta | x, y) | x, \theta^{(i-1)} ]
$
\item Maximization.
Let
$
\theta^{(i)} = \argmax_{\theta} Q(\theta, \theta^{(i-1)})
$
\end{enumerate}
$$
P(y_j=1 | x_j, \theta^{(i-1)}) =
\frac{
\alpha^{(i-1)} \phi(x_j | \mu_1^{(i-1)}, \sigma_1^{2^{(i-1)}})
}
{
\alpha^{(i-1)} \phi(x_j | \mu_1^{(i-1)}, \sigma_1^{2^{(i-1)}})
+
(1-\alpha^{(i-1)}) \phi(x_j | \mu_2^{(i-1)}, \sigma_2^{2^{(i-1)}})
}
$$
$$
Q(\theta, \theta^{(i-1)}) =
\sum_{j=1}^{n}
l(\theta | x_j, 1) P(y_j=1 | x_j, \theta^{(i-1)})
+
l(\theta | x_j, 2) P(y_j=2 | x_j, \theta^{(i-1)})
$$

General case:
$$
P(X | \theta) = \prod_{i=1}^{n} p(x_i | \theta) = L(\theta | X)
$$
$$
Z = (X, Y), z_i = (x_i, y_i)
$$
$$
P(x, y | \theta) = P(y | x, \theta) P(X | \theta)
$$
We can justify that
$$
0 \leq
Q(\theta^{(i)}, \theta^{(i-1)}) -
Q(\theta^{(i-1)}, \theta^{(i-1)})
=
E\left(
\log\frac{
P(Y | X, \theta^{(i)})
} {
P(Y | X, \theta^{(i-1)})
}
\right)
+ \log \frac{
f(X | \theta^{(i)})
}{
f(X | \theta^{(i-1)})
}
$$
but
$
E\left(
\log\frac{
P(Y | X, \theta^{(i)})
} {
P(Y | X, \theta^{(i-1)})
}
\right)
\leq 0
$,
so 
$
\log \frac{
f(X | \theta^{(i)})
}{
f(X | \theta^{(i-1)})
}
\geq 0
$

For the general mixture model
$
\alpha_1, \alpha_2, \dots, \alpha_M
$
whose sum equals $1$ correspond to the probability that
$x_i \sim \theta_i$.

$$
Q(\theta, \theta^{(k)}) =
\sum_{l=1}^{M}
\sum_{i=1}^{N}
\log(\alpha_l)P(y_i=l | x_i, \theta^{(k)})
+
\sum_{l=1}^{M}
\sum_{i=1}^{N}
\log[(\alpha_l)P_l( x_i | \theta_l)] P(y_i=l | x_i, \theta^{(k)})
$$

e.g. 2 coins,
coin1
$
\left\{
\begin{array}{ll}
H & p
\\
T & 1-p
\end{array}
\right.
$,
coin2
$
\left\{
\begin{array}{ll}
H & q
\\
T & 1-q
\end{array}
\right.
$,
and toss coin1 with $\pi$ probability.
This model is unidentifiable. The total numbers of $H$ and $T$ is a sufficient statistic, so we can estimate the probability of total $H$ and total $T$, but never $p$ or $q$.

Stein's estimator

suppose $x_i \sim N(\mu_i, 1), i=1,\dots, n$ independent.
$\mu=(\mu_1, \dots, \mu_n)$ is unknown.
$l(\mu, d) = \sum_{i=1}^{n} (\mu_i-d_i)^2$.
\\
We have a naive estimator $\hat{\mu}_i = x_i$.
\\
For $n=1$, $\hat{\mu} = x$ is minimax, and is admissible.
\\
For general $n\geq 3$, $\hat{\mu}=x$ is minimax, but not admissible.

Stein's estimator
$
\hat{\mu} =
(1-\frac{n-2}{||x||_2^2})x
$
dominates the naive estimator. Surprisingly, it is also admissible.

$$
\hat{\mu}_1 =
x_1\left(1-\frac{n-2}{x_1^2+x_2^2+\cdots+x_n^2}\right)
$$
Why we take into account some other observations independent from $x_1$, because the loss function entangled the $\mu_i$'s together!

Suppose we replace $\hat{\mu} = X$ with $X+\lambda(X)$
$$
E\left[||X+\lambda(X)-\mu||_2^2\right]
=
\int \dots \int ||X+\lambda(X)-\mu||_2^2 \Psi_{\mu,1}(X)dX
=
n + E_{\mu} (2 \nabla \lambda(X) + ||\lambda(X)||_2^2)
$$
We call
$
2 \nabla \lambda(X) + ||\lambda(X)||_2^2
$
unbiased estimator of the Risk. If it is always negative, we can improve from naive estimator.

$$
\lambda(X) = -\frac{cX}{||X||_2^2}
=
c \frac{(x_1, x_2, \dots, x_n)}{x_1^2 + \cdots + x_n^2}
$$
$$
\frac{\partial\lambda}{\partial x_i}
=
-c \frac{1}{x_1^2+\cdots+x_n^2}+c\frac{2 x_1^2}{x_1^2+\cdots+x_n^2}
$$
So $\forall c \in (0, 2(n-1))$, shrinking to zero by factor $c$, or even shrinking to any point by factor $c$ can improve over the naive estimator. ($0$ is not special as the center)

Even more, the linear regression $Y=X\beta+\epsilon$,
$\hat{\beta} = (X'X)^{-1}X'Y$.
If we shrink, we can get better result. In more detail, let $z$ be the eigen vector of $X\beta$, and we multiply both sides by $z$, then the equation becomes the usual $(Yz)_i \sim N((X\beta z)_i, (\epsilon z)_i)$ problem, and we should shrink $Yz$.

This is called the stein's phenomenon.
\end{document}
