\documentclass[11pt]{article}
\usepackage{amsmath, graphicx, amsfonts}
\def\thestudent{Zhilei Xu (929552018)}
\def\theprob{3}
\usepackage{fancyhdr}
\usepackage{lastpage}
\pagestyle{fancy}
\fancyhf{}
\fancyhf[HLEO]{\thestudent}
\fancyhf[HCEO]{18.466 - Problem Set \theprob}
\fancyhf[HREO]{Page \thepage\ of \pageref{LastPage}}
\renewcommand\headrulewidth{0.4pt}
\newcommand\erf{\textrm{erf}}
\newcommand\sgn{\textrm{sgn}}
\newcommand{\widesim}[2][1.5]{
  \mathrel{\overset{#2}{\scalebox{#1}[1]{$\sim$}}}
}
\newcommand\eqnlabel[1]{\label{eqn:#1}}
\newcommand\eqnref[1]{(\ref{eqn:#1})}
\newcommand{\Beta}{\mathcal{B}}
\newcommand{\ProbS}{\iftrue}
\newcommand{\ProbE}{\fi}
\begin{document}
\section{Bickel and Docksum, p. 197, problem 2}
\ProbS
Let $X_1, \dots, X_n$ be the indicators of $n$ Bernoulli trials with success probability $\theta$. Suppose $l(\theta, a)$ is the quadratic loss $(\theta-a)^2$ and that the prior $\pi(\theta)$ is the beta,
$\beta(r, s)$, density. Find the Bayes estimate $\hat{\theta}_B$ of
$\theta$ and write it as a weighted average $w\theta_0 + (1-w)\bar{X}$ of the mean $\theta_0$ of the prior and the sample mean $\bar{X} = S/n$. Show that $\hat{\theta}_B = (S+1)/(n+2)$ for the uniform prior.
\ProbE

$
f(X, \theta) = \prod_{i=1}^{n} \theta^{X_i} (1-\theta)^{1-X_i}
= \theta^{S} (1-\theta)^{n-S},
\pi(\theta) = \theta^{r-1} (1-\theta)^{s-1} / B(r,s)
$
\\
$
\pi_X(\theta) \propto
{\theta^{(S+r)-1}(1-\theta)^{(n-S+s)-1}} / {B(r, s)} \propto
{\theta^{(S+r)-1}(1-\theta)^{(n-S+s)-1}} / {B(S+r, n-S+s)}
% \cdot 
%({B(S+r, n-S+s)} / B(r,s))
%\frac{\Gamma(r+s)\Gamma(S+r)\Gamma(n-S+s)}{\Gamma(r)\Gamma(s)\Gamma(n+r+s)}
$
,\\
hence $\pi_X(\theta)$ must be the pdf of a
$\beta(S+r, n-S+s)$ distribution. So
$
\hat{\theta}_B =
\int_{0}^{1} \theta \pi_X(\theta) d\theta
= \frac{S+r}{n+r+s}
= \frac{r+s}{r+s+n} \cdot \frac{r}{r+s} +
  \frac{n}{r+s+n} \cdot \frac{S}{n}
= w \theta_0 + (1-w) \bar{X}
$
,\\
where $w = \frac{r+s}{r+s+n}$,
is the Bayes estimate of $\theta$.

For the uniform prior, for some constant parameter
$-\infty < u<v < \infty$,
$
\pi(\theta) = 1/(v-u)
$
, and
$$
\pi_X(\theta) \propto
f(X, \theta) =
\theta^{S} (1-\theta)^{n-S}
=
\theta^{(S+1)-1} (1-\theta)^{(n-S+1)-1} \propto
\frac{\theta^{(S+1)-1} (1-\theta)^{(n-S+1)-1}}{B(S+1, n-S+1)}
$$
thus $\pi_X(\theta)$ must be the pdf of a
$\beta(S+1, n-S+1)$ distribution.\\
So
$
\hat{\theta}_B =
\int_{0}^{1} \theta \pi_X(\theta) d\theta
= (S+1)/(n+2)
$
for the uniform prior.

\section{Problem 2}
\ProbS
For $X$ having a binomial $(n,p)$ distribution, we have as estimators of $p$ the classical unbiased and maximum likelihood estimator
$
\hat{p} := X/n
$, and we have the Bayes estimate for a uniform prior,
$
p_B := (X+1)/(n+2)
$. For $n=20$, find for what values of $p$ each of these has smaller mean-squared error.
\ProbE

$$
a = MSE[\hat{p}] = Var[\hat{p}] = Var[X] / n^2 = p(1-p)/n
$$
$$
Var[{p}_B] = Var[X+1] / (n+2)^2 = np(1-p)/(n+2)^2
$$
$$
Bias[{p}_B] = (np+1)/(n+2)-p
= (1-2p)/(n+2),
Bias[{p}_B]^2 = (1-p)^2/(n+2)^2
$$
$$
b = MSE[{p}_B] = (1-p)[1+(n-1)p]/(n+2)^2
$$
$$
d = [n(n+2)^2/(1-p)](a-b) = (n+2)^2 p - n[1+(n-1)p]
= (5n+4)p-n
$$
Clearly when $d<0$, $\hat{p}$ has smaller MSE,
when $d>0$, $p_B$ has smaller MSE,
and when $d=0$, both have the same MSE.
For $n=20$, $d=104p-20$, so
when $0<p<\frac{20}{104} \approx 0.1923$, $\hat{p}$ has smaller MSE,
when $1>p>\frac{20}{104} \approx 0.1923$, $p_B$ has smaller MSE,
and when $p=\frac{20}{104} \approx 0.1923$, both have the same MSE.

When $p=0$ or $p=1$, $\hat{p}$ always has smaller MSE because it has zero MSE while $p_B$ has positive MSE.

\section{Bickel and Docksum, p.203, problem 2}
\ProbS
Note: Jensen's inequality also holds for conditional expectations.

Let $A=R$. We shall say a loss function is \emph{convex}, if
$l(\theta, \alpha a_0 + (1-\alpha)a_1) \leq
\alpha l(\theta, a_0) + (1-\alpha)l(\theta, a_1)$,
for any $a_0, a_1, \theta, 0<\alpha<1$.
Suppose that there is an unbiased estimate $\delta$ of $q(\theta)$
and that $T(\mathbf{X})$ is sufficient.
Show that if $l(\theta, a)$ is convex and
$\delta^{*}(\mathbf{X}) = E(\delta(\mathbf{X}) | t(\mathbf{X}))$,
then $R(\theta, \delta^{*}) \leq R(\theta, \delta)$.

\emph{Hint}: Use \emph{Jensen's inequality}: If $g$ is a convex function and $X$ is a random variable, then $E(g(X)) \geq g(E(X))$.
\ProbE

Because $t(X)$ is sufficient, $p(X | t(X), \theta)$ does not involve $\theta$ at all. So
$$
E_{\theta}[\delta(X) | t(X)] =
\int \delta(X)p(X | t(X), \theta) dX =
\int \delta(X)p(X | t(X)) dX =
E[ \delta(X) | t(X) ] =
\delta^*(X)
$$

Because $l(\theta, \cdot)$ is convex for any $\theta$, and Jensen's inequality,
$$
R(\theta, \delta) =
E_\theta[l(\theta, \delta(X))] =
E_\theta[ E_\theta[l(\theta, \delta(X)) | t(X)] ] \geq
$$
$$
E_\theta[l(\theta, E_\theta[\delta(X) | t(X)])] =
E_\theta[l(\theta, \delta^*(X))] =
R(\theta, \delta^*)
$$
i.e. $R(\theta, \delta^*) \leq R(\theta, \delta)$.

\section{Problem 4}
\ProbS
Consider the family of $N(\theta, 1)$ distributions with the Cauchy prior density
$$
\pi(\theta) = \frac{1}{\pi(1+\theta^2)},
$$
\\
$-\infty < \theta < +\infty$.
Suppose given one observation $x$ from $N(\theta, 1)$
and that we want to estimate $\theta$.
Show that the expectation of $|\theta|$ with respect to the prior is infinite, but that a Bayes estimator of $\theta$ exists.
(You are not asked to find it in closed form.)
\ProbE

$
\int_{-\infty}^{\infty} |\theta| \pi(\theta) d\theta =
\pi^{-1} \int_{-\infty}^{\infty} |\theta| (1+\theta^2)^{-1} d\theta =
2\pi^{-1} \int_{0}^{\infty} \theta (1+\theta^2)^{-1} d\theta =
\pi^{-1} \int_{0}^{\infty} (1+t)^{-1}dt =
\pi^{-1} \ln(1+t) |_{0}^{\infty} = +\infty
$
, so the expectation of $|\theta|$ w.r.t. the prior is infinite.
On the other hand,\\
\begin{align}
\pi_{x}(\theta) =
\pi^{-1} \frac{1}{1+\theta^2} \frac{1}{\sqrt{2\pi}} e^{-\frac{(\theta-x)^2}{2}}
\\
\int_{-\infty}^{\infty} |\theta| \pi_{x}(\theta) d\theta =
\pi^{-1} \int_{-\infty}^{\infty} \frac{1}{1+\theta^2} \frac{1}{\sqrt{2\pi}} e^{-\frac{(\theta-x)^2}{2}}
\end{align}
\\
Because

\section{Problem 5}
\ProbS
The Jeffreys prior (Bickel and Docksum, p. 203, Problem 15, but with a part omitted and others added). (By the way this prior was invented by H. Jeffreys, so it should not be called ``Jeffrey's prior.'')
Suppose we have a parametric family of probability mass functions or densities with a likelihood function $f(\theta, x)$, defined for $\theta$ in an open interval $(a,b)$ where $-\infty \leq a < b \leq +\infty$.
The \emph{Fisher information} of the family, at a given $\theta$, is defined as
$$
I(\theta) = E_{\theta}[(\partial \log f(\theta, x) / \partial \theta)^2]
$$
(``Information inequalities'' handout, non-numbered display in the middle of p. 3). The \emph{Jeffreys prior} measure $\Pi_{J}$ on $(a,b)$ has a density
$
j(\theta) := \sqrt{I(\theta)}
$, so that for
$a \leq c < d \leq b$,
$$
\Pi_{J}((c,d)) = \int_{c}^{d} j(\theta) d\theta
$$
This may be improper (have infinite total measure), but if it is finite, the \emph{Jeffreys prior probability} is found by normalizing it.
\ProbE

\subsection*{(a)}
\ProbS
Show that for $(a,b) = (-\infty, \infty)$, and $\theta = \mu$, for the family of $N(\mu, 1)$ densities, the Jeffreys prior density is a positive constant and therefore, the prior is improper (has total measure $+\infty$).
\ProbE

\subsection*{(b)}
\ProbS
In the general case, suppose we have a parameter $\psi$ satisfying
$u < \psi < v$ for some $u,v$ with $-\infty \leq u < v \leq +\infty$,
and a continuously differentiable function $h$ from $(u,v)$ onto $(a,b)$ with $h'(x)>0$ for all $x \in (u,v)$.
Then $g(\psi, x) = f(h(\psi), x)$ gives a reparameterization of the same family of densities.
Bickel and Doksum, Problem 3 p. 203, parts (a) and (b), ask one to show that the Fisher information $I(\theta)$ is not equivariant, i.e. as defined with repect to $f(\cdot, x)$ at $\theta$, it may not be the same as $I(\psi)$ defined with respect to $g(\psi, x)$ with $h(\psi)=\theta$, but the information inequality lower bound is equivariant.
This problem does not ask you to prove either of those things, rather, it asks you to show that the Jeffreys prior measure is equivariant, in fact, it is invariant under the kind of transformation we're considering, as follows.
Let $j_f$ be a Jeffreys prior density as defined above with respect to $f(\theta, x)$, and let $j_g$ be the Jeffreys prior dnesity defined with respect to $g(\psi, x)$. Then show that for any $\xi, \eta$ with $u < \xi < \eta < v$, we have
$$
\int_{\xi}^{\eta} j_g(\psi)d\psi = \int_{h(\xi)}^{h(\eta)} j_f(\theta) d\theta.
$$
\emph{Hint}: this is a change of variables in an integral, with use of the chain rule. If you prefer, do the special case in part (d) before this general case.
\ProbE

\subsection*{(c)}
\ProbS
Consider the Bernoulli $(p)$ distributions for $0<p<1$, with likelihood function $p^X(1-p)^{1-X}$ where $X=0$ or $1$. Evaluate its Jeffreys prior probability density (Bickel and Doksum state what it is for a binomial distribution with general $n$). This is a density in a known family, namely what?
\ProbE

\subsection*{(d)}
\ProbS
Suppose we consider the same distribution as in (c) but with the exponential family parameterization where $\theta = \log(p/(1-p)), p=e^{\theta}/(1+e^{\theta})$. Find explicitly the Jeffreys prior density with respect to $\theta$ (for it, part (b) would then hold for $\psi = \theta$).
\ProbE

\subsection*{(e)}
\ProbS
In ``Information inequalities,'' Proposition 4, it is shown that if a family of distributions $P_{\theta}$ has a finite Fisher information $I_1(\theta)$ and a regularity condition (2) holds, then the Fisher information for $n$ i.i.d. variables with a distribution in the family also has finite Fisher information $I_{n}(\theta) = nI_1(\theta)$.
For $X_1, \dots, X_n$ i.i.d. Bernoulli $(p)$, $S_n = X_1 + \cdots + X_n$ is a sufficient statistic, why?
Then $S_n$ has a binomial $(n, p)$ distribution. So, check that as Bickel and Doksum say, the Jeffreys prior probability density for this distribution for any $n$ is the same as for $n=1$ found in (c).
\ProbE
\end{document}
